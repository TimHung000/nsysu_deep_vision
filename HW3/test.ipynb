{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(object):\n",
    "  def __init__(self, device='cpu'):\n",
    "    self.W = None # classifier weights\n",
    "    self.dv = device\n",
    "  def save_weights(self, path):\n",
    "    torch.save(self.W, path)\n",
    "  def load_weights(self, path):\n",
    "    self.W = torch.load(path)\n",
    "  def sigmoid(self, score):\n",
    "    prob = torch.zeros_like(score)\n",
    "    ####################\n",
    "    # TODO:\n",
    "    # 1. Implement sigmoid function on `score` and store in variable `prob`\n",
    "    ####################\n",
    "    # -----START OF YOUR CODE-----\n",
    "    prob = 1 / (1 + np.exp(-score))\n",
    "\n",
    "    # ------END OF YOUR CODE------\n",
    "    return prob\n",
    "  def forward(self, x):\n",
    "    num_data, data_dim = x.shape\n",
    "    if self.W is None:\n",
    "      np.random.seed(0)\n",
    "      self.W = torch.from_numpy(np.random.randn(data_dim)*1e-4).to(self.dv)\n",
    "    x = x.to(self.dv)\n",
    "    pred_y = torch.zeros(num_data).to(self.dv)\n",
    "    ####################\n",
    "    # TODO:\n",
    "    # 1. Implement linear classifier f(x) = W * x, and then transform the predicted values to probabilities by sigmoid function\n",
    "    # 2. Store probabilities in variable `pred_y`\n",
    "    ####################\n",
    "    # -----START OF YOUR CODE----- *\n",
    "    pred_y = torch.matmul(self.W, x.transpose(0,1))\n",
    "    pred_y = self.sigmoid(pred_y)\n",
    "    \n",
    "    # ------END OF YOUR CODE------\n",
    "    self.cache = (x, pred_y)\n",
    "    return pred_y\n",
    "  def backward(self, dL):\n",
    "    dL = dL.to(self.dv)\n",
    "    dW = torch.zeros(1).to(self.dv)\n",
    "    ####################\n",
    "    # TODO:\n",
    "    # 1. Derive the gradients of weights, calculate their average and store it in variable `dW`\n",
    "    #   HINT 1: Chain rule d(loss)/d(weight) = d(loss)/d(pred_y) * d(pred_y)/d(weight)\n",
    "    #   HINT 2: Use self.cache (batch flatten data x that multiplied weights in forward process, preditcted output that calculated by weights and input)\n",
    "    ####################\n",
    "    # -----START OF YOUR CODE-----\n",
    "    dSigmoid = self.cache[1] * (1 - self.cache[1])\n",
    "    dz = self.cache[0]\n",
    "    dw = torch.matmul((dL * dSigmoid).reshape(1, -1), self.cache[0]) / self.cache[1].shape[0]\n",
    "    # dw = torch.matmul(a, self.cache[0])\n",
    "    \n",
    "    # ------END OF YOUR CODE------\n",
    "    return dW\n",
    "\n",
    "class BCEloss(object):\n",
    "  def __init__(self, device='cpu'):\n",
    "    self.dv = device\n",
    "  def __call__(self, y, pred_y):\n",
    "    y = y.to(self.dv)\n",
    "    pred_y = pred_y.to(self.dv)\n",
    "    L = torch.zeros_like(y).to(self.dv)\n",
    "    dL = torch.zeros_like(y).to(self.dv)\n",
    "    ####################\n",
    "    # TODO:\n",
    "    # 1. Implement binary cross-entropy loss and store the results of each data in variable `L`\n",
    "    # 2. Derive the gradient of binary cross-entropy loss and store the results of each data in variable `dL`\n",
    "    ####################\n",
    "    # -----START OF YOUR CODE-----\n",
    "    L = -(y * np.log(pred_y) + (1-y) * np.log(1 - pred_y))\n",
    "\n",
    "    dL = (pred_y - y) / pred_y * (1 - pred_y)\n",
    "    # ------END OF YOUR CODE------\n",
    "    return L, dL\n",
    "\n",
    "class Optimizer(object):\n",
    "  def __init__(self, model, learning_rate):\n",
    "    self.model = model\n",
    "    self.lr = learning_rate\n",
    "  def step(self, dW):\n",
    "    dW = dW.to(model.dv)\n",
    "    new_weights = self.model.W.clone()\n",
    "    ####################\n",
    "    # TODO:\n",
    "    # 1. Update model weights by gradient descent and store it in variable `new_weights`\n",
    "    ####################\n",
    "    # -----START OF YOUR CODE-----\n",
    "    new_weights = new_weights - self.lr * dW\n",
    "    \n",
    "    # ------END OF YOUR CODE------\n",
    "    self.model.W = new_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected -1.249252247115394 but got [0.]\n"
     ]
    }
   ],
   "source": [
    "def rel_err(a, b):\n",
    "    return torch.max(torch.abs(a - b) / (torch.maximum(torch.tensor(1e-8), torch.abs(a) + torch.abs(b))))\n",
    "\n",
    "DEVICE='cpu'\n",
    "model = LinearClassifier(DEVICE)\n",
    "# Model_Tests(model)\n",
    "x = torch.tensor([1, 2, 3, 4, 5, 6], dtype=torch.float64).reshape(3, 2)\n",
    "y = torch.tensor([0, 1, 1], dtype=torch.float64).to(model.dv)\n",
    "\n",
    "pred_y = model.forward(x)\n",
    "L = torch.pow(y - pred_y, 2) # using MSE loss\n",
    "dL = 2 * pred_y - 2 * y\n",
    "val = model.backward(dL)\n",
    "truth = -1.249252247115394\n",
    "if (rel_err(torch.tensor(truth).to(model.dv), val) < 1e-6).cpu().numpy():\n",
    "    print(f'Correct')\n",
    "else:\n",
    "    print(f'Expected {truth} but got {val.cpu().numpy()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "class_deep_vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
